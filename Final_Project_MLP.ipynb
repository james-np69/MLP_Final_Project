{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WMJwOZS3X9cE"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import cv2"
      ],
      "metadata": {
        "id": "meOo1k4To3AL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dataset(batch_size, input_image, output_label):\n",
        "    # Convert the inputs to a Dataset.\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((input_image, output_label))\n",
        "\n",
        "    # Resize and normalize images in a lambda function\n",
        "    def process_images(image, label):\n",
        "        image = tf.image.resize(image, [224, 224], method=tf.image.ResizeMethod.BICUBIC)\n",
        "        image = (image - 125.0) / 255.0\n",
        "        return image, label\n",
        "\n",
        "    # Apply the image processing function to all images\n",
        "    dataset = dataset.map(process_images, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "    # Shuffle and batch the dataset\n",
        "    dataset = dataset.shuffle(buffer_size=1024).batch(batch_size)\n",
        "\n",
        "    # Prefetch for performance\n",
        "    dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "    return dataset"
      ],
      "metadata": {
        "id": "L9Fe1kqYmiQY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(x_train,y_train),(x_test,y_test)=cifar10.load_data()"
      ],
      "metadata": {
        "id": "EdAHyu4YtTC5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89482b6e-4273-4977-9acb-dcd281acc0d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170498071/170498071 [==============================] - 3s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model=keras.Sequential(\n",
        "    [\n",
        "     keras.Input(shape=(224,224,3)),\n",
        "     layers.Conv2D(filters=64,kernel_size=3,padding=\"same\",activation=\"relu\"),\n",
        "     layers.BatchNormalization(),\n",
        "     layers.Conv2D(filters=64,kernel_size=3,padding=\"same\",activation=\"relu\"),\n",
        "     layers.BatchNormalization(),\n",
        "     layers.MaxPool2D(pool_size=(2,2)),\n",
        "     layers.Conv2D(filters=128,kernel_size=3,padding=\"same\",activation=\"relu\"),\n",
        "     layers.BatchNormalization(),\n",
        "     layers.Conv2D(filters=128,kernel_size=3,padding=\"same\",activation=\"relu\"),\n",
        "     layers.BatchNormalization(),\n",
        "     layers.MaxPool2D(pool_size=(2,2)),\n",
        "     layers.Conv2D(filters=256,kernel_size=3,padding=\"same\",activation=\"relu\"),\n",
        "     layers.BatchNormalization(),\n",
        "     layers.Conv2D(filters=256,kernel_size=3,padding=\"same\",activation=\"relu\"),\n",
        "     layers.BatchNormalization(),\n",
        "     layers.MaxPool2D(pool_size=(2,2)),\n",
        "     layers.Conv2D(filters=512,kernel_size=3,padding=\"same\",activation=\"relu\"),\n",
        "     layers.BatchNormalization(),\n",
        "     layers.Conv2D(filters=512,kernel_size=3,padding=\"same\",activation=\"relu\"),\n",
        "     layers.BatchNormalization(),\n",
        "     layers.Conv2D(filters=512,kernel_size=3,padding=\"same\",activation=\"relu\"),\n",
        "     layers.BatchNormalization(),\n",
        "     layers.MaxPool2D(pool_size=(2,2)),\n",
        "     layers.Conv2D(filters=512,kernel_size=3,padding=\"same\",activation=\"relu\"),\n",
        "     layers.BatchNormalization(),\n",
        "     layers.Conv2D(filters=512,kernel_size=3,padding=\"same\",activation=\"relu\"),\n",
        "     layers.BatchNormalization(),\n",
        "     layers.Conv2D(filters=512,kernel_size=3,padding=\"same\",activation=\"relu\"),\n",
        "     layers.BatchNormalization(),\n",
        "     layers.MaxPool2D(pool_size=(2,2)),\n",
        "     layers.Flatten(),\n",
        "     layers.Dense(4096,activation=\"relu\"),\n",
        "     layers.Dropout(rate=0.2),\n",
        "     layers.Dense(4096,activation=\"relu\"),\n",
        "     layers.Dropout(rate=0.2),\n",
        "     layers.Dense(10)\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "d1HtRPmJtcUw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tm-CJBDJxOXK",
        "outputId": "e67ef8fc-e0ef-4a6b-e340-2b81a7b82498"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 224, 224, 64)      1792      \n",
            "                                                                 \n",
            " batch_normalization (Batch  (None, 224, 224, 64)      256       \n",
            " Normalization)                                                  \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 224, 224, 64)      36928     \n",
            "                                                                 \n",
            " batch_normalization_1 (Bat  (None, 224, 224, 64)      256       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2  (None, 112, 112, 64)      0         \n",
            " D)                                                              \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 112, 112, 128)     73856     \n",
            "                                                                 \n",
            " batch_normalization_2 (Bat  (None, 112, 112, 128)     512       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 112, 112, 128)     147584    \n",
            "                                                                 \n",
            " batch_normalization_3 (Bat  (None, 112, 112, 128)     512       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPoolin  (None, 56, 56, 128)       0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_4 (Conv2D)           (None, 56, 56, 256)       295168    \n",
            "                                                                 \n",
            " batch_normalization_4 (Bat  (None, 56, 56, 256)       1024      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " conv2d_5 (Conv2D)           (None, 56, 56, 256)       590080    \n",
            "                                                                 \n",
            " batch_normalization_5 (Bat  (None, 56, 56, 256)       1024      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPoolin  (None, 28, 28, 256)       0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_6 (Conv2D)           (None, 28, 28, 512)       1180160   \n",
            "                                                                 \n",
            " batch_normalization_6 (Bat  (None, 28, 28, 512)       2048      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " conv2d_7 (Conv2D)           (None, 28, 28, 512)       2359808   \n",
            "                                                                 \n",
            " batch_normalization_7 (Bat  (None, 28, 28, 512)       2048      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " conv2d_8 (Conv2D)           (None, 28, 28, 512)       2359808   \n",
            "                                                                 \n",
            " batch_normalization_8 (Bat  (None, 28, 28, 512)       2048      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " max_pooling2d_3 (MaxPoolin  (None, 14, 14, 512)       0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_9 (Conv2D)           (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " batch_normalization_9 (Bat  (None, 14, 14, 512)       2048      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " conv2d_10 (Conv2D)          (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " batch_normalization_10 (Ba  (None, 14, 14, 512)       2048      \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " conv2d_11 (Conv2D)          (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " batch_normalization_11 (Ba  (None, 14, 14, 512)       2048      \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " max_pooling2d_4 (MaxPoolin  (None, 7, 7, 512)         0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 25088)             0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 4096)              102764544 \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 4096)              0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 4096)              16781312  \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 4096)              0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 10)                40970     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 133727306 (510.13 MB)\n",
            "Trainable params: 133719370 (510.10 MB)\n",
            "Non-trainable params: 7936 (31.00 KB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size=50\n",
        "epochs=25\n",
        "learningrate=0.01\n",
        "loss_fun=keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "optimizer=keras.optimizers.SGD(learning_rate=learningrate)\n",
        "trainable_dataset=create_dataset(batch_size,x_train,y_train)"
      ],
      "metadata": {
        "id": "k5x42-erxadj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = create_dataset(batch_size, x_train, y_train)\n"
      ],
      "metadata": {
        "id": "0UCf5Rh4jCs4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def train_step(x, y, model, loss_fun, optimizer, accuracy):\n",
        "    with tf.GradientTape() as tape:\n",
        "        logits = model(x, training=True)\n",
        "        loss_value = loss_fun(y, logits)\n",
        "    grads = tape.gradient(loss_value, model.trainable_weights)\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
        "    # Calculate predictions and update accuracy\n",
        "    predictions = tf.argmax(logits, axis=1)\n",
        "    accuracy.update_state(y, predictions)\n",
        "    return loss_value\n",
        "\n",
        "accuracy = tf.keras.metrics.Accuracy()\n",
        "for epoch in range(epochs):\n",
        "    print(\"\\nStart of epoch %d\" % (epoch,))\n",
        "    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
        "        loss_value = train_step(x_batch_train, y_batch_train, model, loss_fun, optimizer, accuracy)\n",
        "        if step % 200 == 0:\n",
        "            print(\n",
        "                \"Training loss (for one batch) at step %d: %.4f\"\n",
        "                % (step, float(loss_value))\n",
        "            )\n",
        "            print(\"Seen so far: %s samples\" % ((step + 1) * batch_size))\n",
        "            print(\"Training accuracy: %.4f\" % float(accuracy.result()))\n",
        "    # Reset the metrics for the next epoch\n",
        "    accuracy.reset_states()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "A6MCORT5V3Jk",
        "outputId": "c6c1f5ae-502e-48a5-da02-1cbe5d28c798"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Start of epoch 0\n",
            "Training loss (for one batch) at step 0: 3.7808\n",
            "Seen so far: 50 samples\n",
            "Training accuracy: 0.1200\n",
            "Training loss (for one batch) at step 200: 1.2771\n",
            "Seen so far: 10050 samples\n",
            "Training accuracy: 0.3550\n",
            "Training loss (for one batch) at step 400: 1.5463\n",
            "Seen so far: 20050 samples\n",
            "Training accuracy: 0.4049\n",
            "Training loss (for one batch) at step 600: 1.4062\n",
            "Seen so far: 30050 samples\n",
            "Training accuracy: 0.4427\n",
            "Training loss (for one batch) at step 800: 1.3248\n",
            "Seen so far: 40050 samples\n",
            "Training accuracy: 0.4688\n",
            "\n",
            "Start of epoch 1\n",
            "Training loss (for one batch) at step 0: 1.0751\n",
            "Seen so far: 50 samples\n",
            "Training accuracy: 0.6400\n",
            "Training loss (for one batch) at step 200: 1.3119\n",
            "Seen so far: 10050 samples\n",
            "Training accuracy: 0.6101\n",
            "Training loss (for one batch) at step 400: 0.9090\n",
            "Seen so far: 20050 samples\n",
            "Training accuracy: 0.6218\n",
            "Training loss (for one batch) at step 600: 0.8647\n",
            "Seen so far: 30050 samples\n",
            "Training accuracy: 0.6362\n",
            "Training loss (for one batch) at step 800: 0.9622\n",
            "Seen so far: 40050 samples\n",
            "Training accuracy: 0.6458\n",
            "\n",
            "Start of epoch 2\n",
            "Training loss (for one batch) at step 0: 0.8683\n",
            "Seen so far: 50 samples\n",
            "Training accuracy: 0.7000\n",
            "Training loss (for one batch) at step 200: 0.8395\n",
            "Seen so far: 10050 samples\n",
            "Training accuracy: 0.7279\n",
            "Training loss (for one batch) at step 400: 0.9383\n",
            "Seen so far: 20050 samples\n",
            "Training accuracy: 0.7313\n",
            "Training loss (for one batch) at step 600: 0.7368\n",
            "Seen so far: 30050 samples\n",
            "Training accuracy: 0.7403\n",
            "Training loss (for one batch) at step 800: 0.5976\n",
            "Seen so far: 40050 samples\n",
            "Training accuracy: 0.7465\n",
            "\n",
            "Start of epoch 3\n",
            "Training loss (for one batch) at step 0: 0.7059\n",
            "Seen so far: 50 samples\n",
            "Training accuracy: 0.7400\n",
            "Training loss (for one batch) at step 200: 0.5511\n",
            "Seen so far: 10050 samples\n",
            "Training accuracy: 0.7966\n",
            "Training loss (for one batch) at step 400: 0.4311\n",
            "Seen so far: 20050 samples\n",
            "Training accuracy: 0.8039\n",
            "Training loss (for one batch) at step 600: 0.3906\n",
            "Seen so far: 30050 samples\n",
            "Training accuracy: 0.8106\n",
            "Training loss (for one batch) at step 800: 0.5227\n",
            "Seen so far: 40050 samples\n",
            "Training accuracy: 0.8166\n",
            "\n",
            "Start of epoch 4\n",
            "Training loss (for one batch) at step 0: 0.4410\n",
            "Seen so far: 50 samples\n",
            "Training accuracy: 0.8400\n",
            "Training loss (for one batch) at step 200: 0.4380\n",
            "Seen so far: 10050 samples\n",
            "Training accuracy: 0.8582\n",
            "Training loss (for one batch) at step 400: 0.3242\n",
            "Seen so far: 20050 samples\n",
            "Training accuracy: 0.8653\n",
            "Training loss (for one batch) at step 600: 0.3984\n",
            "Seen so far: 30050 samples\n",
            "Training accuracy: 0.8706\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-291d40c1572e>\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nStart of epoch %d\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx_batch_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch_train\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mloss_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m200\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             print(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 832\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    833\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    866\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 868\u001b[0;31m       return tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    869\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m       )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1321\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1322\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1325\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1484\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1485\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1486\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1487\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1488\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = create_dataset(batch_size, x_test, y_test)\n"
      ],
      "metadata": {
        "id": "6QdhfseX-xtU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_step(x, y, model, loss_fun,  accuracy):\n",
        "    logits = model(x, training=False)\n",
        "    loss_value = loss_fun(y, logits)\n",
        "\n",
        "    # Calculate predictions and update accuracy\n",
        "    predictions = tf.argmax(logits, axis=1)\n",
        "    accuracy.update_state(y, predictions)\n",
        "    return loss_value\n",
        "\n",
        "\n",
        "accuracy = tf.keras.metrics.Accuracy()\n",
        "for epoch in range(epochs):\n",
        "    accuracy_epoch=0\n",
        "    number_batches=0\n",
        "    print(\"\\nStart of epoch %d\" % (epoch,))\n",
        "    for step, (x_batch_test, y_batch_test) in enumerate(test_dataset):\n",
        "        loss_value = test_step(x_batch_test, y_batch_test, model, loss_fun, accuracy)\n",
        "        if step % 1 == 0:\n",
        "            # print(\n",
        "            #     \"test loss (for one batch) at step %d: %.4f\"\n",
        "            #     % (step, float(loss_value))\n",
        "            # )\n",
        "            # print(\"Seen so far: %s samples\" % ((step + 1) * batch_size))\n",
        "            accuracy_epoch+= float(accuracy.result())\n",
        "            number_batches+=1\n",
        "\n",
        "    # Reset the metrics for the next epoch\n",
        "    print(\"acuracy in each epochs\",accuracy_epoch/number_batches)\n",
        "    accuracy.reset_states()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h1QPPjql5t0c",
        "outputId": "a4fb90f0-bf76-48d7-f148-a3b1aa75effc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Start of epoch 0\n",
            "acuracy in each epochs 0.7845233634114266\n",
            "\n",
            "Start of epoch 1\n",
            "acuracy in each epochs 0.7833083656430244\n",
            "\n",
            "Start of epoch 2\n",
            "acuracy in each epochs 0.7853406476974487\n",
            "\n",
            "Start of epoch 3\n",
            "acuracy in each epochs 0.7823028919100762\n",
            "\n",
            "Start of epoch 4\n",
            "acuracy in each epochs 0.7843613344430923\n",
            "\n",
            "Start of epoch 5\n",
            "acuracy in each epochs 0.7821042340993881\n",
            "\n",
            "Start of epoch 6\n",
            "acuracy in each epochs 0.7841131588816643\n",
            "\n",
            "Start of epoch 7\n",
            "acuracy in each epochs 0.7822969123721123\n",
            "\n",
            "Start of epoch 8\n",
            "acuracy in each epochs 0.7812519335746765\n",
            "\n",
            "Start of epoch 9\n",
            "acuracy in each epochs 0.7820750480890274\n",
            "\n",
            "Start of epoch 10\n",
            "acuracy in each epochs 0.7806635066866875\n",
            "\n",
            "Start of epoch 11\n",
            "acuracy in each epochs 0.7822392645478249\n",
            "\n",
            "Start of epoch 12\n",
            "acuracy in each epochs 0.7819165906310082\n",
            "\n",
            "Start of epoch 13\n",
            "acuracy in each epochs 0.781324745118618\n",
            "\n",
            "Start of epoch 14\n",
            "acuracy in each epochs 0.7841784811019897\n",
            "\n",
            "Start of epoch 15\n",
            "acuracy in each epochs 0.782395147383213\n",
            "\n",
            "Start of epoch 16\n",
            "acuracy in each epochs 0.7816811856627465\n",
            "\n",
            "Start of epoch 17\n",
            "acuracy in each epochs 0.7815273797512055\n",
            "\n",
            "Start of epoch 18\n",
            "acuracy in each epochs 0.7846742421388626\n",
            "\n",
            "Start of epoch 19\n",
            "acuracy in each epochs 0.7825691109895706\n",
            "\n",
            "Start of epoch 20\n",
            "acuracy in each epochs 0.7811328402161598\n",
            "\n",
            "Start of epoch 21\n",
            "acuracy in each epochs 0.7833414584398269\n",
            "\n",
            "Start of epoch 22\n",
            "acuracy in each epochs 0.7817745611071587\n",
            "\n",
            "Start of epoch 23\n",
            "acuracy in each epochs 0.7843397146463394\n",
            "\n",
            "Start of epoch 24\n",
            "acuracy in each epochs 0.7838920494914055\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = create_dataset(batch_size, x_test, y_test)"
      ],
      "metadata": {
        "id": "ZZRYDcBT5NOh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Difference is batch normalization dropout feature between layers. which help in converging models in less number of interations.\n",
        "https://github.com/tensorflow/models/tree/master/research/slim (github link)"
      ],
      "metadata": {
        "id": "bzkqEZODLCxj"
      }
    }
  ]
}